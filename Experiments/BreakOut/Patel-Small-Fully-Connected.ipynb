{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the small fully connected network from Patel et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train a fully connected network with one hidden layer of 1000 neurons on the Breakout problem as in the paper by Patel et al. [1]. For a detailed description of how to train DQNs with our code, see the tutorial in the CartPole experiments chapter 1-DQN-Training. In this notebook, we only outline the relevant changes to the CartPole problem in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected PyNN version 0.9.5 and Neo version 0.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# hack to perform relative imports\n",
    "sys.path.append('../../')\n",
    "from Code import train_agent, SQN, FullyConnected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Attention:</b> If the directory with the specified name already exists, the next cell will throw an error. You need to specify a different name or delete the old directory. If this happens, you should restart the kernel, as the directory is a relative path which changes everytime this cell is run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the Result Directory\n",
    "os.chdir('./../../Results/')\n",
    "# choose the name of the result directory\n",
    "result_directory = 'Breakout-DQN-Training-Patel'\n",
    "# create the result directory (throws an error if the directory already exists)\n",
    "os.makedirs(result_directory)\n",
    "os.chdir(result_directory)\n",
    "# for the first experiment we create an additinonal sub folder\n",
    "os.makedirs('DQN')\n",
    "os.chdir('DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducibility (these are not the same as used to create the graphic in our thesis).\n",
    "torch_seed = 12345678\n",
    "torch.manual_seed(torch_seed)\n",
    "random_seed = 12345678\n",
    "random.seed(random_seed)\n",
    "gym_seed = 12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the hyperparameters of the neural network. There are some differences to the CartPole problem:\n",
    "1. Most importantly, the agent does not receive the raw pixels as input, but some preprocessing happens. We use the grayscale preprocessing analog to Patel et al. This preprocessing is specified in a function and then passed to the agent.\n",
    "2. BreakOut uses some additional hyperparameters. Update Frequency determines after how many steps in the environment the networks are updated. Frame skip describes how often an action the agent selected is repeated where the agent does not see any of the in between states. No-op-max describes the maximum number of \"Do-nothing\" operations executed in the beginning of each episode. We use an initial replay memory size where before reaching this size, no training happens.\n",
    "3. The target update in BreakOut happens every x iterations instead of every x episodes.\n",
    "4. There is no notion of solving the BreakOut environment.\n",
    "5. Patel et al. do not define a maximum number of steps per episode, so we simply set it to infinity. However this can only be done if the final random action probability is sufficiently high, such that the agent cannot get stuck, because it never uses \"fire\" to get a new ball into play.\n",
    "6. We use the optimizer RMS-Prop as as Patel et al. use the same hyperparameters as Mnih et al. [2] who use RMS-Prop. However, as PyTorchs RMS-Prop optimizer has slightly different hyperparameters compared to tensorflows RMS-Prop used in Mnih et al., the optimizers might still be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-de4288bf2574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mGYM_TARGET_DURATION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Patel et al. do not specify a limit on the maximum number of steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mMAX_STEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# number of episodes to train the agent as specified by Patel et al.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mNUM_EPISODES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#CartPole\n",
    "env = 'BreakOut-Deterministic-v4'\n",
    "\n",
    "#hyperparameters, set analog to Patel et al.\n",
    "BATCH_SIZE = 32\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.9999977 # 0.9999977 makes epsilon reach 0.1 after approximately 1000000 frames\n",
    "TARGET_UPDATE_FREQUENCY = 10000\n",
    "# variable that tells the agent whether to update the target net every x episodes or every x iterations\n",
    "TARGET_UPDATE_MODE = 'iterations'\n",
    "LEARNING_RATE = 0.00025\n",
    "REPLAY_MEMORY_SIZE = 2*10**5\n",
    "# additional BreakOut hyperparameters\n",
    "UPDATE_FREQUENCY = 4\n",
    "FRAMESKIP = 4\n",
    "NO_OP_MAX = 30\n",
    "# minimum size of the replay memory before the training starts\n",
    "INITIAL_REPLAY_SIZE = 50000\n",
    "\n",
    "# there is no notion of a gym standard for Breakout\n",
    "GYM_TARGET_AVG = None\n",
    "GYM_TARGET_DURATION = None\n",
    "# Patel et al. do not specify a limit on the maximum number of steps\n",
    "MAX_STEPS = torch.inf\n",
    "# number of episodes to train the agent as specified by Patel et al. \n",
    "NUM_EPISODES = 30000\n",
    "# whether to use Double Q Learning and Gradient Clipping\n",
    "DOUBLE_Q = False\n",
    "GRADIENT_CLIPPING = True\n",
    "# whether to render the environment\n",
    "RENDER = True\n",
    "\n",
    "# device: automatically runs on GPU, if a GPU is detected, else uses CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# input is 80x80, one hidden layer with 1000 neurons\n",
    "architecture = [6400, 1000, 4]\n",
    "policy_net = FullyConnected(architecture).to(device)\n",
    "\n",
    "target_net = FullyConnected(architecture).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LEARNING_RATE,eps=0.001,alpha=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(env,policy_net,target_net,BATCH_SIZE,DISCOUNT_FACTOR,EPSILON_START,\n",
    "            EPSILON_END,EPSILON_DECAY,TARGET_UPDATE_FREQUENCY,optimizer,LEARNING_RATE,\n",
    "            REPLAY_MEMORY_SIZE,device,GYM_TARGET_AVG,GYM_TARGET_DURATION,num_episodes=NUM_EPISODES,\n",
    "            max_steps=MAX_STEPS,render=RENDER,double_q_learning=DOUBLE_Q,gradient_clipping=GRADIENT_CLIPPING,\n",
    "            initial_replay_size=INITIAL_REPLAY_SIZE,gym_seed=gym_seed, torch_seed=torch_seed, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Devdhar Patel, Hananel Hazan, Daniel J. Saunders, Hava T. Siegelmann, and Robert Kozma. Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game. Neural Networks, 2019.\n",
    "\n",
    "[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu Joel Veness, Marc G. Bellemare, Alex Graves,  Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, Volume 518, 26 February 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForReportingNew",
   "language": "python",
   "name": "minimalvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
