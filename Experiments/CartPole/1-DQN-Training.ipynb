{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Training\n",
    "This is the first experiment using the CartPole environment. The experiments in CartPole act as a tutorial on how to use our code and all the other experiments are based upon them, so this provides a good starting point if you want to run your own experiments. \n",
    "The tutorials build upon each other and it is recommended to follow them in the following order:\n",
    "1. DQN-Training (How to train a conventional DQN and a spiking DQN using Surrogate Gradients (DSQN).)\n",
    "2. Load-DQN (How to load a previously saved D(S)QN and how to save a replay dataset.)\n",
    "3. Train-Classifier (How to train a spiking or non-spiking classifier on the saved replay data set.)\n",
    "4. SNN-Conversion (How to convert a DQN and a Classifier to a SNN.)\n",
    "5. Load in NEST (How to load a converted or directly trained spiking network in NEST.)\n",
    "6. Conversion in pyNN with NEST or SpyNNaker (How to load spiking network in pyNN using NEST or SpyNNaker as backend.)\n",
    "\n",
    "In this first part we train a DQN (non-spiking and spiking) on the CartPole problem. At the same time this file serves as a tutorial for training DQNs with any environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# hack to perform relative imports\n",
    "sys.path.append('../../')\n",
    "from datetime import date\n",
    "from Code import train_agent, SQN, FullyConnected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with setting up the result directory.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Attention:</b> If the directory with the specified name already exists, this will throw an error. You need to specify a different name or delete the old directory. If this happens, you should restart the kernel, as the directory is a relative path which changes everytime this cell is run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the Result Directory\n",
    "#os.chdir('../../Results/')\n",
    "# choose the name of the result directory\n",
    "#result_directory = 'CartPole-Experiment2-DQN-Training'\n",
    "# create the result directory (throws an error if the directory already exists)\n",
    "#os.makedirs(result_directory)\n",
    "#os.chdir(result_directory)\n",
    "# for the first experiment we create an additinonal sub folder\n",
    "#os.makedirs('DQN')\n",
    "#os.chdir('DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('.')\n",
    "if not any('result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'result_' + str(result_id) + '_DQN_{}'.format(str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the seeds, hyperparameters and initial weights to values that reached the Open AI gym standard succesfully. This should make the results reproducible if you install the virtual environment specified in requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_seed = 135\n",
    "#torch.manual_seed(torch_seed)\n",
    "#random_seed = 795\n",
    "#random.seed(random_seed)\n",
    "#gym_seed = 975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the environment and all the hyperparameters for a non-spiking Q-network. We then set up the result directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole\n",
    "env = 'CartPole-v0'\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 128\n",
    "discount_factor = 0.999\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.999\n",
    "target_update_frequency = 10\n",
    "learning_rate = 0.001\n",
    "replay_memory_size = 4*10**4\n",
    "\n",
    "# minimum size of the replay memory before the training starts\n",
    "initial_replay_size = 0\n",
    "\n",
    "# the gym standard for CartPole (\"solving\" it) is to achieve a 100-episode average of <=195 for 100 consecutive episodes\n",
    "GYM_TARGET_AVG = 195\n",
    "GYM_TARGET_DURATION = 100\n",
    "\n",
    "max_steps = 1000\n",
    "num_episodes = 1000\n",
    "n_runs = 5\n",
    "\n",
    "double_q = False\n",
    "gradient_clipping = False\n",
    "render = False\n",
    "\n",
    "architecture = [4,16,16,2]\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the neural network for the problem: We use a Fully-Connected network with two hidden layers with 16 neurons each and ReLu activations. The size of the in- and output layers is determined by the environment. \n",
    "The target net initially is a copy of the policy net.\n",
    "Then we set up the optimizer: We use Adam using the specified learning and rate and the standard parameters for everything else.\n",
    "The seed fixes the inital weight. Instad, we could also save the initial weights of a network (commented lines below definition of policy net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize policy and target net\n",
    "# this creates a network with 4 input neurons, two hidden layers with 16 neurons each and two output neurons\n",
    "\n",
    "#policy_net = FullyConnected(architecture).to(device)\n",
    "# initial weights are actually fixed through the seed\n",
    "# load the fixed initial weights, remove this line to get random initial weights, not necessary if torch seed is specified\n",
    "#policy_net.load_state_dict(torch.load('./../../CartPole-v0/DQN/initial/model.pt'))\n",
    "\n",
    "#target_net = FullyConnected(architecture).to(device)\n",
    "#target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# initialize optimizer\n",
    "#optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the agent with the specified hyperparameters using the function from Code/train_agent. In the plot, blue shows the individual reward on each episode, while orange shows the average reward over the last 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akl-ma/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 54.85\t Epsilon: 0.05\n",
      "Episode 200\tAverage Score: 185.87\t Epsilon: 0.05\n",
      "Episode 300\tAverage Score: 203.53\t Epsilon: 0.05\n",
      "Episode 400\tAverage Score: 180.20\t Epsilon: 0.05\n",
      "Episode 500\tAverage Score: 142.80\t Epsilon: 0.05\n",
      "Episode 600\tAverage Score: 136.24\t Epsilon: 0.05\n",
      "Episode 700\tAverage Score: 198.65\t Epsilon: 0.05\n",
      "Episode 800\tAverage Score: 393.05\t Epsilon: 0.05\n",
      "Episode 900\tAverage Score: 710.61\t Epsilon: 0.05\n",
      "Episode 1000\tAverage Score: 671.23\t Epsilon: 0.05\n",
      "Best 100 episode average:  753.11  reached at episode  909 . Model saved in folder best.\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 50.07\t Epsilon: 0.05\n",
      "Episode 200\tAverage Score: 165.08\t Epsilon: 0.05\n",
      "Episode 300\tAverage Score: 139.09\t Epsilon: 0.05\n",
      "Episode 400\tAverage Score: 136.53\t Epsilon: 0.05\n",
      "Episode 500\tAverage Score: 125.20\t Epsilon: 0.05\n",
      "Episode 600\tAverage Score: 143.40\t Epsilon: 0.05\n",
      "Episode 700\tAverage Score: 141.67\t Epsilon: 0.05\n",
      "Episode 800\tAverage Score: 634.08\t Epsilon: 0.05\n",
      "Episode 900\tAverage Score: 342.42\t Epsilon: 0.05\n",
      "Episode 1000\tAverage Score: 751.89\t Epsilon: 0.05\n",
      "Best 100 episode average:  751.89  reached at episode  1000 . Model saved in folder best.\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 71.84\t Epsilon: 0.05\n",
      "Episode 200\tAverage Score: 198.97\t Epsilon: 0.05\n",
      "Episode 300\tAverage Score: 152.35\t Epsilon: 0.05\n",
      "Episode 400\tAverage Score: 97.55\t Epsilon: 0.055\n",
      "Episode 500\tAverage Score: 101.96\t Epsilon: 0.05\n",
      "Episode 600\tAverage Score: 164.70\t Epsilon: 0.05\n",
      "Episode 700\tAverage Score: 135.30\t Epsilon: 0.05\n",
      "Episode 800\tAverage Score: 466.86\t Epsilon: 0.05\n",
      "Episode 900\tAverage Score: 886.24\t Epsilon: 0.05\n",
      "Episode 1000\tAverage Score: 818.22\t Epsilon: 0.05\n",
      "Best 100 episode average:  902.37  reached at episode  985 . Model saved in folder best.\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 115.66\t Epsilon: 0.05\n",
      "Episode 200\tAverage Score: 359.61\t Epsilon: 0.05\n",
      "Episode 300\tAverage Score: 36.52\t Epsilon: 0.055\n",
      "Episode 400\tAverage Score: 134.56\t Epsilon: 0.05\n",
      "Episode 500\tAverage Score: 189.07\t Epsilon: 0.05\n",
      "Episode 600\tAverage Score: 162.33\t Epsilon: 0.05\n",
      "Episode 700\tAverage Score: 317.18\t Epsilon: 0.05\n",
      "Episode 800\tAverage Score: 989.17\t Epsilon: 0.05\n",
      "Episode 900\tAverage Score: 757.34\t Epsilon: 0.05\n",
      "Episode 1000\tAverage Score: 290.12\t Epsilon: 0.05\n",
      "Best 100 episode average:  996.37  reached at episode  825 . Model saved in folder best.\n",
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 125.61\t Epsilon: 0.05\n",
      "Episode 154\tAverage Score: 216.73\t Epsilon: 0.05\r"
     ]
    }
   ],
   "source": [
    "smoothed_scores_all = []\n",
    "torch_seeds = [829, 861, 632, 130, 743]\n",
    "random_seeds = [440, 791, 721, 563, 950]\n",
    "gym_seeds = [127, 691, 157, 534, 217]\n",
    "for i_run in range(n_runs):\n",
    "    #torch_seed = random.randint(0, 1000)\n",
    "    torch_seed = torch_seeds[i_run]\n",
    "    torch.manual_seed(torch_seed)\n",
    "    #random_seed = random.randint(0, 1000)\n",
    "    random_seed = random_seeds[i_run]\n",
    "    random.seed(random_seed)\n",
    "    #gym_seed = random.randint(0, 1000)\n",
    "    gym_seed = gym_seeds[i_run]\n",
    "    \n",
    "    policy_net = FullyConnected(architecture).to(device)\n",
    "    target_net = FullyConnected(architecture).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    scores, smoothed_scores = train_agent(env, policy_net, target_net, batch_size,\n",
    "                                          discount_factor, epsilon_start, epsilon_end,\n",
    "                                          epsilon_decay, target_update_frequency, optimizer,\n",
    "                                          learning_rate, replay_memory_size, device, i_run,\n",
    "                                          result_dir, num_episodes=num_episodes,\n",
    "                                          max_steps=max_steps, render=render,\n",
    "                                          double_q_learning=double_q,\n",
    "                                          gradient_clipping=gradient_clipping,\n",
    "                                          initial_replay_size=initial_replay_size,\n",
    "                                          gym_seed=gym_seed, torch_seed=torch_seed,\n",
    "                                          random_seed=random_seed)\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_{}'.format(i_run), smoothed_scores)\n",
    "    \n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_all.append(smoothed_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the training progress of the model. Once the Open AI gym standard is reached, the model is saved\n",
    "in the result directory as trained/model.pt. Additionally, before the training starts the initial weights and the hyperparameters are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores of individual runs\n",
    "for i in range(len(smoothed_scores_all)):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(smoothed_scores_all[i])\n",
    "    plt.ylim(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (median)\n",
    "mean_smoothed_scores = np.mean(smoothed_scores_all, axis=0)\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(smoothed_scores_all[0])), np.nanmedian(smoothed_scores_all, axis=0))\n",
    "plt.fill_between(range(len(smoothed_scores_all[0])), np.nanpercentile(smoothed_scores_all, 2, axis=0),\n",
    "                 np.nanpercentile(smoothed_scores_all, 97, axis=0), alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (mean)\n",
    "mean_smoothed_scores = np.mean(smoothed_scores_all, axis=0)\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(smoothed_scores_all[0])), mean_smoothed_scores)\n",
    "plt.fill_between(range(len(smoothed_scores_all[0])), np.nanpercentile(smoothed_scores_all, 2, axis=0),\n",
    "                 np.nanpercentile(smoothed_scores_all, 97, axis=0), alpha=0.25)\n",
    "plt.ylim(0, 1000)\n",
    "plt.savefig(result_dir + '/DQN_training.png', dpi=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of a spiking DQN or DSQN\n",
    "Next, we train a DSQN using the same hyperparameters as far as possible. That is, all hyperparameters are the same, but we need to define some additional hyperparameters. We adapted the surrogate gradient algorithm we use for the direct training from the SpyTorch jupyter notebooks (available from https://github.com/fzenke/spytorch as of 06.12.2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set up a new sub directory\n",
    "os.chdir('./..')\n",
    "os.makedirs('DSQN')\n",
    "os.chdir('DSQN')\n",
    "# We use a non-leaky integrate-and-fire neuron\n",
    "ALPHA = 0\n",
    "BETA = 1\n",
    "# Simulation time is chosen relatively short, such that the network does not need too much time to run, but not too short,\n",
    "# such that it can still learn something\n",
    "SIMULATION_TIME = 20\n",
    "# We also have to define the input/output and reset methods, to our knowledge, SpyTorch supports only potential outputs \n",
    "# and reset-by-subtraction. As input method we use constant input currents. It would be interesting to see if SpyTorch\n",
    "# can also use reset-to-zero, as this would make it more similar to the iaf_delta models in NEST and SpyNNaker\n",
    "ENCODING = 'constant'\n",
    "DECODING = 'potential'\n",
    "RESET = 'subtraction'\n",
    "# SpyTorch uses a fixed threshold of one, we didn't test other thresholds, but should be possible\n",
    "THRESHOLD = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again set the seeds in a way such that the gym standard is reached. We obtained those seed from our experiments, where we saved the seeds upon succesfully reaching the gym standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively use the seeds gym: 240, torch: 18, random: 626 and a learning rate of 0.0005 \n",
    "# to get the same results as figure 4.9 in the thesis\n",
    "torch.manual_seed(467)\n",
    "random.seed(208)\n",
    "gym_seed = 216"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the neural network. Note, that SpyTorch does not support biases, so we instead add a constant input to each observation (equivalent to first layer biases) and add one additional neuron to each hidden layer to compensate for the missing biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = [4,17,17,2]\n",
    "policy_net = SQN(architecture,device,alpha=ALPHA,beta=BETA,simulation_time=SIMULATION_TIME,add_bias_as_observation=True,\n",
    "                  encoding=ENCODING,decoding=DECODING,reset=RESET,threshold=THRESHOLD)\n",
    "# load the fixed initial weights, remove this line to get random initial weights\n",
    "#policy_net.load_state_dict(torch.load('./../../CartPole-v0/DSQN-Surrogate-Gradients/initial/model.pt'))\n",
    "\n",
    "target_net = SQN(architecture,device,alpha=0,beta=1,simulation_time=SIMULATION_TIME,add_bias_as_observation=True,\n",
    "                  encoding=ENCODING,decoding=DECODING,reset=RESET,threshold=THRESHOLD)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, the agent is trained. The function we use, is exactly the same as for the DQN, the only difference is that the model we pass now is an instance of our SQN class, rather than the PyTorch neural network base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(env,policy_net,target_net,BATCH_SIZE,DISCOUNT_FACTOR,EPSILON_START,\n",
    "            EPSILON_END,EPSILON_DECAY,TARGET_UPDATE_FREQUENCY,optimizer,LEARNING_RATE,\n",
    "            REPLAY_MEMORY_SIZE,device,GYM_TARGET_AVG,GYM_TARGET_DURATION,num_episodes=NUM_EPISODES,\n",
    "            max_steps=MAX_STEPS,render=RENDER,double_q_learning=DOUBLE_Q,gradient_clipping=GRADIENT_CLIPPING,\n",
    "            initial_replay_size=INITIAL_REPLAY_SIZE,gym_seed=gym_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot again shows the rewards in each episode (blue) and the average reward over the last 100 episodes (orange).\n",
    "\n",
    "The next experiment in this series is Load-DQN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
