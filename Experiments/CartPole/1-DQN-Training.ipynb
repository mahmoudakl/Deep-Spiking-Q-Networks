{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Training\n",
    "This is the first experiment using the CartPole environment. The experiments in CartPole act as a tutorial on how to use our code and all the other experiments are based upon them, so this provides a good starting point if you want to run your own experiments. \n",
    "The tutorials build upon each other and it is recommended to follow them in the following order:\n",
    "1. DQN-Training (How to train a conventional DQN and a spiking DQN using Surrogate Gradients (DSQN).)\n",
    "2. Load-DQN (How to load a previously saved D(S)QN and how to save a replay dataset.)\n",
    "3. Train-Classifier (How to train a spiking or non-spiking classifier on the saved replay data set.)\n",
    "4. SNN-Conversion (How to convert a DQN and a Classifier to a SNN.)\n",
    "5. Load in NEST (How to load a converted or directly trained spiking network in NEST.)\n",
    "6. Conversion in pyNN with NEST or SpyNNaker (How to load spiking network in pyNN using NEST or SpyNNaker as backend.)\n",
    "\n",
    "In this first part we train a DQN (non-spiking and spiking) on the CartPole problem. At the same time this file serves as a tutorial for training DQNs with any environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hack to perform relative imports\n",
    "sys.path.append('../../')\n",
    "from datetime import date\n",
    "from Code import train_agent, SQN, FullyConnected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with setting up the result directory.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Attention:</b> If the directory with the specified name already exists, this will throw an error. You need to specify a different name or delete the old directory. If this happens, you should restart the kernel, as the directory is a relative path which changes everytime this cell is run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Directory result_1_DQN_20201119 to store the results in\n"
     ]
    }
   ],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('.')\n",
    "if not any('result' in d and 'DQN' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'result' in d and 'DQN' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'result_' + str(result_id) + '_DQN_{}'.format(str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the seeds, hyperparameters and initial weights to values that reached the Open AI gym standard succesfully. This should make the results reproducible if you install the virtual environment specified in requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the environment and all the hyperparameters for a non-spiking Q-network. We then set up the result directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole\n",
    "env = 'CartPole-v0'\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 128\n",
    "discount_factor = 0.999\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 0.999\n",
    "target_update_frequency = 10\n",
    "learning_rate = 0.001\n",
    "replay_memory_size = 4*10**4\n",
    "\n",
    "# minimum size of the replay memory before the training starts\n",
    "initial_replay_size = 0\n",
    "\n",
    "max_steps = 1000\n",
    "num_episodes = 1000\n",
    "n_runs = 5\n",
    "\n",
    "double_q = False\n",
    "gradient_clipping = False\n",
    "render = False\n",
    "\n",
    "architecture = [4, 16, 16, 2]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the neural network for the problem: We use a Fully-Connected network with two hidden layers with 16 neurons each and ReLu activations. The size of the in- and output layers is determined by the environment. \n",
    "The target net initially is a copy of the policy net.\n",
    "Then we set up the optimizer: We use Adam using the specified learning and rate and the standard parameters for everything else.\n",
    "The seed fixes the inital weight. Instad, we could also save the initial weights of a network (commented lines below definition of policy net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the agent with the specified hyperparameters using the function from Code/train_agent. In the plot, blue shows the individual reward on each episode, while orange shows the average reward over the last 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_seeds = [random.randint(0, 1000) for _ in range(n_runs)]\n",
    "random_seeds = [random.randint(0, 1000) for _ in range(n_runs)]\n",
    "gym_seeds = [random.randint(0, 1000) for _ in range(n_runs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_scores_dqn_all = []\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    torch_seed = torch_seeds[i_run]\n",
    "    random_seed = random_seeds[i_run]\n",
    "    gym_seed = gym_seeds[i_run]\n",
    "    \n",
    "    torch.manual_seed(torch_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    policy_net = FullyConnected(architecture).to(device)\n",
    "    target_net = FullyConnected(architecture).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    scores, smoothed_scores = train_agent(env, policy_net, target_net, batch_size,\n",
    "                                          discount_factor, epsilon_start, epsilon_end,\n",
    "                                          epsilon_decay, target_update_frequency, optimizer,\n",
    "                                          learning_rate, replay_memory_size, device, i_run,\n",
    "                                          result_dir, num_episodes=num_episodes,\n",
    "                                          max_steps=max_steps, render=render,\n",
    "                                          double_q_learning=double_q,\n",
    "                                          gradient_clipping=gradient_clipping,\n",
    "                                          initial_replay_size=initial_replay_size,\n",
    "                                          gym_seed=gym_seed, torch_seed=torch_seed,\n",
    "                                          random_seed=random_seed)\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_{}'.format(i_run), smoothed_scores)\n",
    "    \n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dqn_all.append(smoothed_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the training progress of the model. Once the Open AI gym standard is reached, the model is saved\n",
    "in the result directory as trained/model.pt. Additionally, before the training starts the initial weights and the hyperparameters are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores of individual runs\n",
    "for i in range(len(smoothed_scores_dqn_all)):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(smoothed_scores_dqn_all[i])\n",
    "    plt.ylim(0, 1000)\n",
    "    plt.savefig(result_dir + '/training_dqn_{}.png'.format(i), dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (mean)\n",
    "mean_smoothed_scores = np.mean(smoothed_scores_dqn_all, axis=0)\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(smoothed_scores_dqn_all[0])), mean_smoothed_scores)\n",
    "plt.fill_between(range(len(smoothed_scores_dqn_all[0])),\n",
    "                 np.nanpercentile(smoothed_scores_dqn_all, 2, axis=0),\n",
    "                 np.nanpercentile(smoothed_scores_dqn_all, 97, axis=0), alpha=0.25)\n",
    "plt.ylim(0, 1000)\n",
    "plt.savefig(result_dir + '/DQN_training.png', dpi=1000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of a spiking DQN or DSQN\n",
    "Next, we train a DSQN using the same hyperparameters as far as possible. That is, all hyperparameters are the same, but we need to define some additional hyperparameters. We adapted the surrogate gradient algorithm we use for the direct training from the SpyTorch jupyter notebooks (available from https://github.com/fzenke/spytorch as of 06.12.2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Directory result_1_DSQN_20201119 to store the results in\n"
     ]
    }
   ],
   "source": [
    "dirs = os.listdir('.')\n",
    "if not ('result' in d and 'DSQN' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'result' in d and 'DSQN' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'result_' + str(result_id) + '_DSQN_{}'.format(str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a non-leaky integrate-and-fire neuron\n",
    "ALPHA = 0\n",
    "BETA = 1\n",
    "\n",
    "# Simulation time is chosen relatively short, such that the network does not need too much\n",
    "# time to run, but not too short, such that it can still learn something\n",
    "SIMULATION_TIME = 20\n",
    "\n",
    "# We also have to define the input/output and reset methods, to our knowledge, SpyTorch\n",
    "# supports only potential outputs  and reset-by-subtraction. As input method we use constant\n",
    "# input currents. It would be interesting to see if SpyTorch can also use reset-to-zero, as\n",
    "# this would make it more similar to the iaf_delta models in NEST and SpyNNaker\n",
    "ENCODING = 'constant'\n",
    "DECODING = 'potential'\n",
    "RESET = 'subtraction'\n",
    "\n",
    "# SpyTorch uses a fixed threshold of one, we didn't test other thresholds, but should\n",
    "# be possible\n",
    "THRESHOLD = 1\n",
    "\n",
    "architecture = [4, 16, 16, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the neural network. Note, that SpyTorch does not support biases, so we instead add a constant input to each observation (equivalent to first layer biases) and add one additional neuron to each hidden layer to compensate for the missing biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, the agent is trained. The function we use, is exactly the same as for the DQN, the only difference is that the model we pass now is an instance of our SQN class, rather than the PyTorch neural network base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akl-ma/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 15.08\t Epsilon: 0.24\n",
      "Episode 200\tAverage Score: 43.03\t Epsilon: 0.05\n",
      "Episode 300\tAverage Score: 93.99\t Epsilon: 0.05\n",
      "Episode 400\tAverage Score: 75.21\t Epsilon: 0.05\n",
      "Episode 500\tAverage Score: 262.36\t Epsilon: 0.05\n",
      "Episode 600\tAverage Score: 145.61\t Epsilon: 0.05\n",
      "Episode 700\tAverage Score: 148.07\t Epsilon: 0.05\n",
      "Episode 800\tAverage Score: 340.57\t Epsilon: 0.05\n",
      "Episode 900\tAverage Score: 717.49\t Epsilon: 0.05\n",
      "Episode 932\tAverage Score: 618.33\t Epsilon: 0.05\r"
     ]
    }
   ],
   "source": [
    "smoothed_scores_dsqn_all = []\n",
    "for i_run in range(n_runs):\n",
    "    torch_seed = torch_seeds[i_run]\n",
    "    random_seed = random_seeds[i_run]\n",
    "    gym_seed = gym_seeds[i_run]\n",
    "    \n",
    "    torch.manual_seed(torch_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    policy_net = SQN(architecture, device, alpha=ALPHA, beta=BETA,\n",
    "                 simulation_time=SIMULATION_TIME, add_bias_as_observation=False,\n",
    "                 encoding=ENCODING, decoding=DECODING, reset=RESET, threshold=THRESHOLD)\n",
    "    target_net = SQN(architecture, device, alpha=0, beta=1, simulation_time=SIMULATION_TIME,\n",
    "                 add_bias_as_observation=False, encoding=ENCODING, decoding=DECODING,\n",
    "                 reset=RESET, threshold=THRESHOLD)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    scores, smoothed_scores = train_agent(env, policy_net, target_net, batch_size,\n",
    "                                          discount_factor, epsilon_start, epsilon_end,\n",
    "                                          epsilon_decay, target_update_frequency, optimizer,\n",
    "                                          learning_rate, replay_memory_size, device, i_run,\n",
    "                                          result_dir, num_episodes=num_episodes,\n",
    "                                          max_steps=max_steps, render=render,\n",
    "                                          double_q_learning=double_q,\n",
    "                                          gradient_clipping=gradient_clipping,\n",
    "                                          initial_replay_size=initial_replay_size,\n",
    "                                          gym_seed=gym_seed, torch_seed=torch_seed,\n",
    "                                          random_seed=random_seed)\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dsqn_all.append(smoothed_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot again shows the rewards in each episode (blue) and the average reward over the last 100 episodes (orange).\n",
    "\n",
    "The next experiment in this series is Load-DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores of individual runs\n",
    "for i in range(len(smoothed_scores_dqn_all)):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(smoothed_scores_dqn_all[i])\n",
    "    plt.ylim(0, 1000)\n",
    "    plt.savefig(result_dir + '/training_dsqn_{}.png'.format(i), dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_smoothed_scores = np.mean(smoothed_scores_dsqn_all, axis=0)\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(smoothed_scores_dsqn_all[0])), mean_smoothed_scores)\n",
    "plt.fill_between(range(len(smoothed_scores_dsqn_all[0])),\n",
    "                 np.nanpercentile(smoothed_scores_dsqn_all, 2, axis=0),\n",
    "                 np.nanpercentile(smoothed_scores_dsqn_all, 97, axis=0), alpha=0.25)\n",
    "plt.ylim(0, 1000)\n",
    "plt.savefig(result_dir + '/DSQN_training.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_smoothed_scores_dqn = np.mean(smoothed_scores_dqn_all, axis=0)\n",
    "mean_smoothed_scores_dsqn = np.mean(smoothed_scores_dqn_all, axis=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(range(len(smoothed_scores_dqn_all[0])), mean_smoothed_scores, color='b')\n",
    "plt.fill_between(range(len(smoothed_scores_dqn_all[0])),\n",
    "                 np.nanpercentile(smoothed_scores_dqn_all, 2, axis=0),\n",
    "                 np.nanpercentile(smoothed_scores_dqn_all, 97, axis=0), alpha=0.25)\n",
    "\n",
    "plt.plot(range(len(smoothed_scores_dsqn_all[0])), mean_smoothed_scores, color='r')\n",
    "plt.fill_between(range(len(smoothed_scores_dsqn_all[0])),\n",
    "                 np.nanpercentile(smoothed_scores_dsqn_all, 2, axis=0),\n",
    "                 np.nanpercentile(smoothed_scores_dsqn_all, 97, axis=0), alpha=0.25)\n",
    "\n",
    "plt.ylim(0, 1000)\n",
    "plt.savefig(result_dir + '/DQN_vs_DSQN_training.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code import SQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " policy_net = SQN(architecture, device, alpha=ALPHA, beta=BETA,\n",
    "                 simulation_time=SIMULATION_TIME, add_bias_as_observation=True,\n",
    "                 encoding=ENCODING, decoding=DECODING, reset=RESET, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envir = gym.make(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = envir.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(obs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.forward(torch.tensor(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
