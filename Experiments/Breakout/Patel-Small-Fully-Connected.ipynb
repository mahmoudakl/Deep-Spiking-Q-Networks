{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the small fully connected network from Patel et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train a fully connected network with one hidden layer of 1000 neurons on the Breakout problem as in the paper by Patel et al. [1]. For a detailed description of how to train DQNs with our code, see the tutorial in the CartPole experiments chapter 1-DQN-Training. In this notebook, we only outline the relevant changes to the CartPole problem in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# hack to perform relative imports\n",
    "sys.path.append('../../')\n",
    "from Code import train_agent, SQN, FullyConnected\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Attention:</b> If the directory with the specified name already exists, the next cell will throw an error. You need to specify a different name or delete the old directory. If this happens, you should restart the kernel, as the directory is a relative path which changes everytime this cell is run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the Result Directory\n",
    "os.chdir('./../../Results/')\n",
    "# choose the name of the result directory\n",
    "result_directory = 'Breakout-Experiment-DQN-Training-Patel'\n",
    "# create the result directory (throws an error if the directory already exists)\n",
    "os.makedirs(result_directory)\n",
    "os.chdir(result_directory)\n",
    "# for the first experiment we create an additinonal sub folder\n",
    "os.makedirs('DQN')\n",
    "os.chdir('DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducibility (these are not the same as used to create the graphic in our thesis).\n",
    "torch_seed = 12345678\n",
    "torch.manual_seed(torch_seed)\n",
    "random_seed = 12345678\n",
    "random.seed(random_seed)\n",
    "gym_seed = 12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the hyperparameters of the neural network. There are some differences to the CartPole problem:\n",
    "1. Most importantly, the agent does not receive the raw pixels as input, but some preprocessing happens. We use the grayscale preprocessing analog to Patel et al. This preprocessing is specified in a function and then passed to the agent.\n",
    "2. BreakOut uses some additional hyperparameters. Update Frequency determines after how many steps in the environment the networks are updated. Frame skip describes how often an action the agent selected is repeated where the agent does not see any of the in between states. No-op-max describes the maximum number of \"Do-nothing\" operations executed in the beginning of each episode. No-op-range is a tuple (x,y) where y is the no-op-max and x is the no-op-min which is equivalent to the history length which is 4. We use an initial replay memory size where before reaching this size, no training happens.\n",
    "3. The target update in BreakOut happens every x iterations instead of every x episodes.\n",
    "4. There is no notion of solving the BreakOut environment.\n",
    "5. Patel et al. do not define a maximum number of steps per episode, so we simply set it to infinity. However this can only be done if the final random action probability is sufficiently high, such that the agent cannot get stuck, because it never uses \"fire\" to get a new ball into play.\n",
    "6. We use the optimizer RMS-Prop as as Patel et al. use the same hyperparameters as Mnih et al. [2] who use RMS-Prop. However, as PyTorchs RMS-Prop optimizer has slightly different hyperparameters compared to tensorflows RMS-Prop used in Mnih et al., the optimizers might still be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CartPole\n",
    "env = 'BreakoutDeterministic-v4'\n",
    "\n",
    "# define the preprocessing function \n",
    "\n",
    "\n",
    "#hyperparameters, set analog to Patel et al.\n",
    "BATCH_SIZE = 32\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.9999977 # 0.9999977 makes epsilon reach 0.1 after approximately 1000000 frames\n",
    "TARGET_UPDATE_FREQUENCY = 10000\n",
    "# variable that tells the agent whether to update the target net every x episodes or every x iterations\n",
    "TARGET_UPDATE_MODE = 'iterations'\n",
    "LEARNING_RATE = 0.00025\n",
    "REPLAY_MEMORY_SIZE = 2*10**5\n",
    "# additional BreakOut hyperparameters\n",
    "UPDATE_FREQUENCY = 4\n",
    "FRAMESKIP = 4\n",
    "NO_OP = 0 # the action which is \"do nothing\"\n",
    "NO_OP_RANGE = (4,30)\n",
    "OBSERVATION_HISTORY_LENGTH = 4\n",
    "# minimum size of the replay memory before the training starts\n",
    "INITIAL_REPLAY_SIZE = 50000\n",
    "\n",
    "# there is no notion of a gym standard for Breakout\n",
    "GYM_TARGET_AVG = None\n",
    "GYM_TARGET_DURATION = None\n",
    "# Patel et al. do not specify a limit on the maximum number of steps\n",
    "MAX_STEPS = np.inf\n",
    "# number of episodes to train the agent as specified by Patel et al. \n",
    "NUM_EPISODES = 30000\n",
    "# whether to use Double Q Learning and Gradient Clipping\n",
    "DOUBLE_Q = False\n",
    "GRADIENT_CLIPPING = True\n",
    "# whether to render the environment\n",
    "RENDER = True\n",
    "\n",
    "# device: automatically runs on GPU, if a GPU is detected, else uses CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# input is 80x80, one hidden layer with 1000 neurons\n",
    "architecture = [6400, 1000, 4]\n",
    "policy_net = FullyConnected(architecture).to(device)\n",
    "\n",
    "target_net = FullyConnected(architecture).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr=LEARNING_RATE,eps=0.001,alpha=0.95)\n",
    "\n",
    "# define input preprocessing: grayscale processing according to Patel\n",
    "def input_preprocessing(observation_history):\n",
    "    def rgb2gray(rgb):\n",
    "        \"\"\"converts a coloured image to gray\"\"\"\n",
    "        # extract the luminance\n",
    "        image = rgb[:, :, 0] * 0.2126 + rgb[:, :, 1] * 0.7152 + rgb[:, :, 2] * 0.0722\n",
    "        # crop the image\n",
    "        image = image[25::, :]\n",
    "        # downsize to 80x80\n",
    "        image_resized = resize(image, (80, 80))\n",
    "        # return the processed image and the new prev frame\n",
    "        return image_resized\n",
    "    preprocessed = rgb2gray(observation_history[0]) + 0.75 * rgb2gray(observation_history[1]) + 0.5 * rgb2gray(observation_history[2]) + 0.25 * rgb2gray(observation_history[3])\n",
    "    # rescale such that maximum value is 255 for conversion to uint8\n",
    "    preprocessed[preprocessed>255] = 255\n",
    "    preprocessed = torch.tensor(preprocessed,device=device,dtype=torch.uint8)\n",
    "    # cast to tensor of shape (6400,) to be processed correctly by the SQN\n",
    "    preprocessed = preprocessed.reshape(6400,)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Warning:</b> The code below is not fully tested, because the computer this was tested on has not enough computational power to complete the training in a reasonable amount of time. If you run into any problems, feel free to contact me via chris.hahn@tum.de. From our thesis it should be expected that the agent achieves a 100-episode reward of around 10 after a couple of thousand episodes and then pretty much stays there for the raining eppisodes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call train agent with the additional options for Breakout\n",
    "train_agent(env,policy_net,target_net,BATCH_SIZE,DISCOUNT_FACTOR,EPSILON_START,\n",
    "            EPSILON_END,EPSILON_DECAY,TARGET_UPDATE_FREQUENCY,optimizer,LEARNING_RATE,\n",
    "            REPLAY_MEMORY_SIZE,device,GYM_TARGET_AVG,GYM_TARGET_DURATION,num_episodes=NUM_EPISODES,\n",
    "            max_steps=MAX_STEPS,render=RENDER,double_q_learning=DOUBLE_Q,gradient_clipping=GRADIENT_CLIPPING,\n",
    "            initial_replay_size=INITIAL_REPLAY_SIZE,gym_seed=gym_seed, torch_seed=torch_seed, random_seed=random_seed,\n",
    "            input_preprocessing=input_preprocessing, no_op_range=NO_OP_RANGE,no_op=NO_OP,\n",
    "            update_frequency=UPDATE_FREQUENCY,observation_history_length=OBSERVATION_HISTORY_LENGTH, \n",
    "           target_update_mode=TARGET_UPDATE_MODE, frameskip=FRAMESKIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue line shows the single episode rewards, thr orange line indicates 100-episode average rewards.\n",
    "\n",
    "Next, we train a DSQN.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Attention:</b> If the directory with the specified name already exists, the next cell will throw an error. You need to specify a different name or delete the old directory. If this happens, you should restart the kernel, as the directory is a relative path which changes everytime this cell is run.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Warning:</b> Again, the code is not fully tested. You can have a look at our thesis to see what kind of results can be expected.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./../DSQN')\n",
    "os.chdir('./../DSQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the network\n",
    "SIMULATION_TIME = 20\n",
    "architecture = [6400,1000,4]\n",
    "policy_net = SQN(architecture, device, alpha=0, beta=1, simulation_time=SIMULATION_TIME,\n",
    "                 add_bias_as_observation=True)\n",
    "target_net = SQN(architecture, device, alpha=0, beta=1, simulation_time=SIMULATION_TIME,\n",
    "                 add_bias_as_observation=True)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "\n",
    "# call train agent with the additional options for Breakout\n",
    "train_agent(env,policy_net,target_net,BATCH_SIZE,DISCOUNT_FACTOR,EPSILON_START,\n",
    "            EPSILON_END,EPSILON_DECAY,TARGET_UPDATE_FREQUENCY,optimizer,LEARNING_RATE,\n",
    "            REPLAY_MEMORY_SIZE,device,GYM_TARGET_AVG,GYM_TARGET_DURATION,num_episodes=NUM_EPISODES,\n",
    "            max_steps=MAX_STEPS,render=RENDER,double_q_learning=DOUBLE_Q,gradient_clipping=GRADIENT_CLIPPING,\n",
    "            initial_replay_size=0,gym_seed=gym_seed, torch_seed=torch_seed, random_seed=random_seed,\n",
    "            input_preprocessing=input_preprocessing, no_op_range=NO_OP_RANGE,no_op=NO_OP,\n",
    "            update_frequency=UPDATE_FREQUENCY,observation_history_length=OBSERVATION_HISTORY_LENGTH, \n",
    "           target_update_mode=TARGET_UPDATE_MODE, frameskip=FRAMESKIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Devdhar Patel, Hananel Hazan, Daniel J. Saunders, Hava T. Siegelmann, and Robert Kozma. Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game. Neural Networks, 2019.\n",
    "\n",
    "[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu Joel Veness, Marc G. Bellemare, Alex Graves,  Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, Volume 518, 26 February 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForReportingNew",
   "language": "python",
   "name": "minimalvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
