{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN-Training\n",
    "Experiment 1-DQN-Training in the CartPole Experiments folder describes in detail, how a DQN is trained. This notebook therefore only contains the relevant code. For additional comments, see the CartPole experiment or check our thesis. We first train a conventional DQN and then a spiking DSQN. Loading and converting is omitted completely for MountainCar. The CartPole experiments can simply be adapted by setting the environment to MountainCar and loading the networks trained in this notebook. The results of conversion and loading the MountainCar networks are reported in our thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# hack to perform relative imports\n",
    "sys.path.append('../../')\n",
    "from Code import train_agent, SQN, FullyConnected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Attention:</b> If the directory with the specified name already exists, this will throw an error. You need to specify a different name or delete the old directory. If this happens, you should restart the kernel, as the directory is a relative path which changes everytime this cell is run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the Result Directory\n",
    "os.chdir('./../../Results/')\n",
    "# choose the name of the result directory\n",
    "result_directory = 'MountainCar-Experiment1-DQN-Training'\n",
    "# create the result directory (throws an error if the directory already exists)\n",
    "os.makedirs(result_directory)\n",
    "os.chdir(result_directory)\n",
    "# for the first experiment we create an additinonal sub folder\n",
    "os.makedirs('DQN')\n",
    "os.chdir('DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducibility\n",
    "torch_seed = 524\n",
    "torch.manual_seed(torch_seed)\n",
    "random_seed = 912\n",
    "random.seed(random_seed)\n",
    "gym_seed = 447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define environment\n",
    "env = 'MountainCar-v0'\n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "DISCOUNT_FACTOR = 0.999\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.999\n",
    "TARGET_UPDATE_FREQUENCY = 10\n",
    "LEARNING_RATE = 0.0005\n",
    "REPLAY_MEMORY_SIZE = 10**3\n",
    "# minimum size of the replay memory before the training starts\n",
    "INITIAL_REPLAY_SIZE = 0\n",
    "# the gym standard for solving MountainCar is to reach a 100 episode average of -110 for 100 consecutive episodes\n",
    "# as this is hard to reach, we relaxed the standard to reach a 100 episode average of -130 for 50 consecutive episodes\n",
    "GYM_TARGET_AVG = -130\n",
    "GYM_TARGET_DURATION = 50\n",
    "# maximum number of steps before the environment is reset\n",
    "MAX_STEPS = 200\n",
    "# number of episodes to train the agent\n",
    "NUM_EPISODES = 1000\n",
    "# whether to use Double Q Learning and Gradient Clipping\n",
    "DOUBLE_Q = True\n",
    "GRADIENT_CLIPPING = True\n",
    "# whether to render the environment\n",
    "RENDER = True\n",
    "\n",
    "# device: automatically runs on GPU, if a GPU is detected, else uses CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize policy and target net\n",
    "# for MountainCar we use a slightly larger network architecture\n",
    "architecture = [2,64,64,3] \n",
    "policy_net = FullyConnected(architecture).to(device)\n",
    "\n",
    "target_net = FullyConnected(architecture).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(env,policy_net,target_net,BATCH_SIZE,DISCOUNT_FACTOR,EPSILON_START,\n",
    "            EPSILON_END,EPSILON_DECAY,TARGET_UPDATE_FREQUENCY,optimizer,LEARNING_RATE,\n",
    "            REPLAY_MEMORY_SIZE,device,GYM_TARGET_AVG,GYM_TARGET_DURATION,num_episodes=NUM_EPISODES,\n",
    "            max_steps=MAX_STEPS,render=RENDER,double_q_learning=DOUBLE_Q,gradient_clipping=GRADIENT_CLIPPING,\n",
    "            initial_replay_size=INITIAL_REPLAY_SIZE,gym_seed=gym_seed, torch_seed=torch_seed, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForReportingNew",
   "language": "python",
   "name": "minimalvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
